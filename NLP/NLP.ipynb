{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0bbe978c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# **Natural Language Processing**\n",
    "## **What is NLP?**\n",
    "**NLP stands for Natural Language Processing**, which is a part of Computer Science, Human language, and Artificial Intelligence. It is the technology that is used by machines to understand, analyse, manipulate, and interpret human's languages. It helps developers to organize knowledge for performing tasks such as translation, automatic summarization, Named Entity Recognition (NER), speech recognition, relationship extraction, and topic segmentation.\n",
    "\n",
    "![alt text](https://lh4.googleusercontent.com/HvOsRDR2W55YFM1ffjeBAmCr1hUVeQlT_h2NZXd_dQCdqiaKTDeWIUg28wmznHhI9WhO2tc6q6DVu5SMmfKtfhiCiEocD82JFWSnpich9okxrfBtbwJynbbe8FgUF2vdT2S0Fb0)\n",
    "\n",
    "## **Applications of NLP**\n",
    "There are the following applications of NLP -\n",
    "1. **Question Answering**\n",
    "\n",
    "Question Answering focuses on building systems that automatically answer the questions asked by humans in a natural language.\n",
    "\n",
    "![alt text](https://lh4.googleusercontent.com/llxua0Kxt4LY6Jq0KtwdoLqVp9VmGYtpU7SXy71KHB4tW7CRGT8tQ2xGmaoTZ3gX0CC5TQgRyuKEQttbUVB30yNQuI7R1HcEJKSKNms8BhqBEVnbst7w2KI6UIGzH81Fev77SWQ)\n",
    "\n",
    "2. **Spam Detection**\n",
    "\n",
    "Spam detection is used to detect unwanted emails getting to a user's inbox.\n",
    "\n",
    "![alt text](https://lh3.googleusercontent.com/OMoXE4C6lwKdM0QE55FjxKu8c6pqWt7-5P1AcGfqQPJuSKGjG0xHqKJNtvH9Fo16DbeQLki8uXGuBiblLYXHQE54Xru-hFysYLc1By3vnrRfD33qA51xx1zNnxANhZlaE5aeh-M)\n",
    "\n",
    "3. **Sentiment Analysis**\n",
    "\n",
    "Sentiment Analysis is also known as opinion mining. It is used on the web to analyse the attitude, behaviour, and emotional state of the sender. This application is implemented through a combination of NLP (Natural Language Processing) and statistics by assigning the values to the text (positive, negative, or natural), identify the mood of the context (happy, sad, angry, etc.)\n",
    "\n",
    "![alt text](https://lh6.googleusercontent.com/ochS5RaVmHfIQyEML5QmtH1G9dXq9Oao3a9K7QuaQ_4Zn7EN5LzVg2Rv1cOJgpoNSdpLWqIZf_2VBeop3cmie3WeG0La9aCc4NkZ5fS_01aAwsJ6ACTi31G_xXe-73rdNISPqiM)\n",
    "\n",
    "4. **Machine Translation**\n",
    "\n",
    "Machine translation is used to translate text or speech from one natural language to another natural language.\n",
    "\n",
    "![alt text](https://lh3.googleusercontent.com/AgvZMNyu7VEKSi2O-1gWlx82EqLCZdT5EmVxF4rEPokG2JJy8fSyPgMEVK5juLDTnkz5WVBQ_Nm6TEmXVrnOg5T2C7dYr2JDbanzPaKOWXINCy0uolVIx-Irez-_0rVl3ao3Eds)\n",
    "\n",
    "5. **Spelling correction**\n",
    "\n",
    "Microsoft Corporation provides word processor software like MS-word, PowerPoint for the spelling correction.\n",
    "\n",
    "![alt text](https://lh5.googleusercontent.com/nvn5xkiQ4lncZKob0z5fyMmBIyPyZZu71ltKvXh8qrc9lauc_et6Lt89drEjngOphnUv2xCLUQRkgry0qYF_Sdw4sw7u0g0sjMCRK9X8akuhGHc3VLYS39J4MwxHspHCiGuHkf4)\n",
    "\n",
    "6. **Speech Recognition**\n",
    "\n",
    "Speech recognition is used for converting spoken words into text. It is used in applications, such as mobile, home automation, video recovery, dictating to Microsoft Word, voice biometrics, voice user interface, and so on.\n",
    "\n",
    "7. **Chatbot**\n",
    "\n",
    "Implementing the Chatbot is one of the important applications of NLP. It is used by many companies to provide the customer's chat services.\n",
    "\n",
    "![alt text](https://lh3.googleusercontent.com/o5A5kbmvoKElNZCsLSlv0aFyWw9Un4tINXuh98RQ-pCkjQvZRujkj7y_XaAZ73BL9wfMZH36ElKb7XOieagpHEJPQSWBLSur2hLAzw7tvypqw7vTqe8kGLv0LabwncV3F3R8glE)\n",
    "\n",
    "8. **Information extraction**\n",
    "\n",
    "Information extraction is one of the most important applications of NLP. It is used for extracting structured information from unstructured or semi-structured machine-readable documents.\n",
    "\n",
    "9. **Natural Language Understanding (NLU)**\n",
    "\n",
    "It converts a large set of text into more formal representations such as first-order logic structures that are easier for the computer programs to manipulate notations of the natural language processing.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db05d41",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "There are various techniques of doing NLP, here we will study the Bag of words model.\n",
    "\n",
    "Whenever we apply any algorithm in NLP, it works on numbers. We cannot directly feed our text into that algorithm. Hence, the Bag of Words model is used to preprocess the text by converting it into a bag of words, which keeps a count of the total occurrences of most frequently used words.\n",
    "\n",
    "## **Problems we face while NLP:**\n",
    "**Categorical data**\n",
    "\n",
    "Since in NLP we deal exclusively with text data, it is very important to handle it as we are already aware that we cannot pass categorical data to the ML model. A ML model works on algorithms which consist of various mathematical and statistical formulas and we cannot pass text as input for a mathematical formula.\n",
    "\n",
    "**No fixed length**\n",
    "\n",
    "In NLP we cannot estimate the length of the data. We have to use free flowing data.\n",
    "\n",
    "## **Flow of Analysing Text data:**\n",
    "\n",
    "![alt text](https://docs.google.com/drawings/u/0/d/s62D0wOlx8O7edOmTbyHsUw/image?w=341&h=356&rev=1&ac=1&parent=1OSwv1l6BFj2A0EBicPrt6iK4Y3p0Y-Lc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as pyplot\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "d78c07c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>Liked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Wow... Loved this place.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Crust is not good.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Not tasty and the texture was just nasty.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Stopped by during the late May bank holiday of...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The selection on the menu was great and so wer...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Review  Liked\n",
       "0                           Wow... Loved this place.      1\n",
       "1                                 Crust is not good.      0\n",
       "2          Not tasty and the texture was just nasty.      0\n",
       "3  Stopped by during the late May bank holiday of...      1\n",
       "4  The selection on the menu was great and so wer...      1"
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=pd.read_csv(\"Restaurant_Reviews.tsv\",sep=\"\\t\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "9947be00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['Wow... Loved this place.', 'Crust is not good.',\n",
       "        'Not tasty and the texture was just nasty.',\n",
       "        'Stopped by during the late May bank holiday off Rick Steve recommendation and loved it.',\n",
       "        'The selection on the menu was great and so were the prices.'],\n",
       "       dtype=object),\n",
       " (1000,))"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=data.iloc[:,0].values\n",
    "x[:5],x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "baa954bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 1)"
      ]
     },
     "execution_count": 296,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y=data.iloc[:,1:2]\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "id": "e44553f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wow    Loved this place \n",
      "Wow Loved this place "
     ]
    }
   ],
   "source": [
    "#Include only necessery characters\n",
    "import re \n",
    "#re.sub() take 1D input only\n",
    "rev=re.sub('[^a-zA-Z]', ' ', 'Wow... Loved this place.')\n",
    "print(rev)\n",
    "text=[]\n",
    "new=[]\n",
    "for words in rev.split(\" \"):\n",
    "    if words!=\"\":\n",
    "        print(words,end=\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "299403fb",
   "metadata": {},
   "source": [
    "In the bag of words technique we depend on keywords to identify whether it is a positive comment or negative.\n",
    "\n",
    "All positive keywords are in a bag of words.\n",
    "\n",
    "### **Step 2:  Get rid of unnecessary words/ symbols.**\n",
    "For this we will use the library of **re (regular expressions)**\n",
    "\n",
    "The **re library** has a function **substitute**, here we will mention the following parameters as follows :\n",
    "\n",
    "What to substitute\n",
    "Replace with what\n",
    "Where\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "id": "223891d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Wow    Loved this place ',\n",
       " 'Crust is not good ',\n",
       " 'Not tasty and the texture was just nasty ',\n",
       " 'Stopped by during the late May bank holiday off Rick Steve recommendation and loved it ',\n",
       " 'The selection on the menu was great and so were the prices ']"
      ]
     },
     "execution_count": 298,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_text=[]\n",
    "for rev in x:\n",
    "    rev=re.sub('[^a-zA-Z]', ' ', rev)\n",
    "    filtered_text.append(rev)\n",
    "x=filtered_text\n",
    "filtered_text[:5]       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8377e133",
   "metadata": {},
   "source": [
    "### **Step 3: Convert text into lowercase and a list of words.**\n",
    "To convert in lowercase we will use the function **lower()** and to convert into a list of words we will use **split()**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "0f8fd98d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['then',\n",
       " 'as',\n",
       " 'if',\n",
       " 'i',\n",
       " 'hadn',\n",
       " 't',\n",
       " 'wasted',\n",
       " 'enough',\n",
       " 'of',\n",
       " 'my',\n",
       " 'life',\n",
       " 'there',\n",
       " 'they',\n",
       " 'poured',\n",
       " 'salt',\n",
       " 'in',\n",
       " 'the',\n",
       " 'wound',\n",
       " 'by',\n",
       " 'drawing',\n",
       " 'out',\n",
       " 'the',\n",
       " 'time',\n",
       " 'it',\n",
       " 'took',\n",
       " 'to',\n",
       " 'bring',\n",
       " 'the',\n",
       " 'check']"
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "altered_text=[]\n",
    "for rev in x:\n",
    "    rev=rev.lower()\n",
    "    rev=rev.split()\n",
    "    altered_text.append(rev)\n",
    "len(altered_text)\n",
    "altered_text[999]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa631b55",
   "metadata": {},
   "source": [
    "### **Step 4: Using stopwords.**\n",
    "Stopwords: These are the unnecessary words that donot have a specific meaning.\n",
    "**Eg. this, is, and, the, a, an etc**\n",
    "\n",
    "We will download these stopwords from the library **nltk (Natural Language Toolkit Library)**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "c783eedb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\s\\anaconda3\\lib\\site-packages (3.8.1)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: click in c:\\users\\s\\anaconda3\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\s\\anaconda3\\lib\\site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\s\\anaconda3\\lib\\site-packages (from nltk) (2023.10.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\s\\anaconda3\\lib\\site-packages (from nltk) (4.65.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\s\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "%pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "4aa95a30",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\S\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "b3ce13d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<WordListCorpusReader in 'C:\\\\Users\\\\S\\\\AppData\\\\Roaming\\\\nltk_data\\\\corpora\\\\stopwords'>\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "print(stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "cc701cd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a about above after again against ain all am an and any are aren aren't as at be because been before being below between both but by can couldn couldn't d did didn didn't do does doesn doesn't doing don don't down during each few for from further had hadn hadn't has hasn hasn't have haven haven't having he he'd he'll her here hers herself he's him himself his how i i'd if i'll i'm in into is isn isn't it it'd it'll it's its itself i've just ll m ma me mightn mightn't more most mustn mustn't my myself needn needn't no nor not now o of off on once only or other our ours ourselves out over own re s same shan shan't she she'd she'll she's should shouldn shouldn't should've so some such t than that that'll the their theirs them themselves then there these they they'd they'll they're they've this those through to too under until up ve very was wasn wasn't we we'd we'll we're were weren weren't we've what when where which while who whom why will with won won't wouldn wouldn't y you you'd you'll your you're yours yourself yourselves you've "
     ]
    }
   ],
   "source": [
    "for words in stopwords.words(\"English\"):\n",
    "    print(words,end=\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9468fca5",
   "metadata": {},
   "source": [
    "### **Step 5: Getting rid of stopwords.**\n",
    "For this we will create an empty **list mybag[]** and store all the useful words i.e words other than the stopwords in that list.\n",
    "\n",
    "**Note:** Some words which are present in stopwords but may be useful. Hence, we will create another list **notstopwords** and store those words in that list.\n",
    "\n",
    "**Eg. not** is present in the list of stopwords. But without ‘not’ the meaning of a review can completely change\n",
    "\n",
    "**The gravy was not good. → (without not) The gravy was good.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "8fc39ba9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['wow', 'loved', 'place'],\n",
       " ['crust', 'not', 'good'],\n",
       " ['not', 'tasty', 'texture', 'nasty'],\n",
       " ['stopped',\n",
       "  'late',\n",
       "  'may',\n",
       "  'bank',\n",
       "  'holiday',\n",
       "  'rick',\n",
       "  'steve',\n",
       "  'recommendation',\n",
       "  'loved'],\n",
       " ['selection', 'menu', 'great', 'prices']]"
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mybag=[]\n",
    "nonstopwords=['not']\n",
    "for data in altered_text:\n",
    "    my_bag=[]\n",
    "    for word in data:\n",
    "        if word not in stopwords.words(\"English\"):\n",
    "            my_bag.append(word)\n",
    "        if word in nonstopwords:\n",
    "            my_bag.append(word)\n",
    "    mybag.append(my_bag)\n",
    "mybag[:5]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf7bac55",
   "metadata": {},
   "source": [
    "### **Step 6: Steaming**\n",
    "Steaming is the process of replacing the words with the original word from which it has originated.\n",
    "\n",
    "**Example: loved, loving → love**\n",
    "\n",
    "For this we use the library **PorterStemmer from nltk**.\n",
    "\n",
    "We will create an empty list to store all the steam words. So that we get a unique list of stem words. We will first create an object of the **PorterStemmer** class and then use a for loop to convert the words from mybag to its stem word and then add it to the **mystembag[]**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "id": "35c083a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "my_stembag=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "id": "79867e9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['wow', 'love', 'place'],\n",
       " ['crust', 'not', 'good'],\n",
       " ['not', 'tasti', 'textur', 'nasti'],\n",
       " ['stop',\n",
       "  'late',\n",
       "  'may',\n",
       "  'bank',\n",
       "  'holiday',\n",
       "  'rick',\n",
       "  'steve',\n",
       "  'recommend',\n",
       "  'love'],\n",
       " ['select', 'menu', 'great', 'price'],\n",
       " ['get', 'angri', 'want', 'damn', 'pho'],\n",
       " ['honeslti', 'tast', 'fresh'],\n",
       " ['potato',\n",
       "  'like',\n",
       "  'rubber',\n",
       "  'could',\n",
       "  'tell',\n",
       "  'made',\n",
       "  'ahead',\n",
       "  'time',\n",
       "  'kept',\n",
       "  'warmer'],\n",
       " ['fri', 'great'],\n",
       " ['great', 'touch']]"
      ]
     },
     "execution_count": 306,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps=PorterStemmer()\n",
    "for data in mybag:\n",
    "    my_short_stem=[]\n",
    "    for word in data:\n",
    "        stemword=ps.stem(word)\n",
    "        if stemword not in my_short_stem:\n",
    "            my_short_stem.append(stemword)\n",
    "    my_stembag.append(my_short_stem)\n",
    "my_stembag[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "id": "cf82cd43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['wow love place',\n",
       " 'crust not good',\n",
       " 'not tasti textur nasti',\n",
       " 'stop late may bank holiday rick steve recommend love',\n",
       " 'select menu great price']"
      ]
     },
     "execution_count": 307,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_review=[]\n",
    "for data in my_stembag:\n",
    "    rev=\" \".join(data)\n",
    "    final_review.append(rev)\n",
    "final_review[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b62aa00",
   "metadata": {},
   "source": [
    "## **sklearn.feature_extraction.text.CountVectorizer**\n",
    "\n",
    "Convert a collection of text documents to a matrix of token counts\n",
    "\n",
    "This implementation produces a sparse representation of the counts using **scipy.sparse.csr_matrix**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "id": "c550661b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. wow great\n",
    "# 2. nice man\n",
    "\n",
    "# corpus = [wow, great, nice, man]\n",
    "\n",
    "# countrvectors  = [wow, great, nice, man] => [0, 1, 2, 3]\n",
    "\n",
    "# nice great wow great = [1, 2, 1, 0]\n",
    "\n",
    "# 1. wow great => [1, 1, 0, 0]\n",
    "# 2. nice man => [0, 0, 1, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "id": "05a05cdc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 309,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv=CountVectorizer()\n",
    "x=cv.fit_transform(final_review).toarray()\n",
    "x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "id": "8ce08b39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1566"
      ]
     },
     "execution_count": 310,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cv.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "id": "f2c7e44b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['absolut', 'absolutley', 'accid', 'accommod', 'accomod',\n",
       "       'accordingli', 'account', 'ach', 'acknowledg', 'across', 'actual',\n",
       "       'ad', 'afford', 'afternoon', 'ago', 'ahead', 'airlin', 'airport',\n",
       "       'ala', 'albondiga', 'allergi', 'almond', 'almost', 'alon', 'also',\n",
       "       'although', 'alway', 'amaz', 'ambianc', 'ambienc', 'amount',\n",
       "       'ampl', 'andddd', 'angri', 'annoy', 'anoth', 'anticip', 'anymor',\n",
       "       'anyon', 'anyth', 'anytim', 'anyway', 'apart', 'apolog', 'app',\n",
       "       'appal', 'appar', 'appeal', 'appet', 'appetit', 'appl', 'approv',\n",
       "       'area', 'arepa', 'aria', 'around', 'array', 'arriv', 'articl',\n",
       "       'ask', 'assur', 'ate', 'atmospher', 'atroci', 'attach', 'attack',\n",
       "       'attent', 'attitud', 'auju', 'authent', 'averag', 'avocado',\n",
       "       'avoid', 'aw', 'away', 'awesom', 'awkward', 'awkwardli', 'ayc',\n",
       "       'az', 'baba', 'babi', 'bachi', 'back', 'bacon', 'bad', 'bagel',\n",
       "       'bakeri', 'baklava', 'ball', 'bamboo', 'banana', 'bank', 'bar',\n",
       "       'bare', 'bargain', 'bartend', 'base', 'basebal', 'basic', 'batch',\n",
       "       'bathroom', 'batter', 'bay', 'bbq', 'bean', 'beat', 'beateou',\n",
       "       'beauti', 'becom', 'beef', 'beer', 'begin', 'behind', 'believ',\n",
       "       'bellagio', 'belli', 'besid', 'best', 'better', 'beyond', 'big',\n",
       "       'bigger', 'biggest', 'bill', 'bing', 'bird', 'biscuit', 'bisqu',\n",
       "       'bit', 'bitch', 'bite', 'black', 'blah', 'blame', 'bland',\n",
       "       'blandest', 'blanket', 'block', 'bloddi', 'bloodi', 'bloodiest',\n",
       "       'blow', 'blown', 'blue', 'boba', 'bode', 'boil', 'bone', 'book',\n",
       "       'boot', 'bore', 'bother', 'bottom', 'bouchon'], dtype=object)"
      ]
     },
     "execution_count": 311,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.get_feature_names_out()[:155]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "id": "98afdff2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1566"
      ]
     },
     "execution_count": 312,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "id": "689c4953",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 313,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(x[1]==1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "id": "c071e4ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9, 1566)"
      ]
     },
     "execution_count": 314,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(x[3]==1),len(x[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "id": "fce61598",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1,\n",
       "       1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1,\n",
       "       0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1,\n",
       "       1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1,\n",
       "       1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1,\n",
       "       1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0,\n",
       "       1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0,\n",
       "       0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0,\n",
       "       1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1,\n",
       "       0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1,\n",
       "       0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1,\n",
       "       1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1,\n",
       "       0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0,\n",
       "       1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0,\n",
       "       0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0,\n",
       "       1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1,\n",
       "       0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1,\n",
       "       0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1,\n",
       "       1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1,\n",
       "       0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1,\n",
       "       1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,\n",
       "       1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0,\n",
       "       0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1,\n",
       "       1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1,\n",
       "       1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0,\n",
       "       0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0,\n",
       "       0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1,\n",
       "       0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1,\n",
       "       0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0,\n",
       "       1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1,\n",
       "       1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1,\n",
       "       1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1,\n",
       "       0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1,\n",
       "       0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0,\n",
       "       0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0,\n",
       "       1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0,\n",
       "       1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1,\n",
       "       0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0,\n",
       "       0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1,\n",
       "       0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 315,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.values.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "id": "48c10e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2,random_state=47,stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "id": "049195fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "rfr=RandomForestClassifier(n_estimators=10 ,max_depth=15,criterion=\"entropy\")\n",
    "svc=SVC(kernel=\"rbf\",C=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "id": "70d1114e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\S\\AppData\\Local\\Temp\\ipykernel_48816\\503672577.py:1: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  model1=rfr.fit(x_train,y_train)\n"
     ]
    }
   ],
   "source": [
    "model1=rfr.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "id": "1a52193c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1,\n",
       "        0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1,\n",
       "        1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "        0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0,\n",
       "        1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1,\n",
       "        0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
       "        1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1,\n",
       "        0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1,\n",
       "        1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1,\n",
       "        1, 0], dtype=int64),\n",
       " array([1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0,\n",
       "        0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0,\n",
       "        1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "        0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "        1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1,\n",
       "        0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "        1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0,\n",
       "        1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "        1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0,\n",
       "        1, 0], dtype=int64))"
      ]
     },
     "execution_count": 319,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y1_pred=model1.predict(x_test)\n",
    "y_test.values.ravel(),y1_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "id": "8ef35d90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training accuracy: 0.8275\n",
      "testing accuracy: 0.725\n"
     ]
    }
   ],
   "source": [
    "print(f\"training accuracy: {rfr.score(x_train,y_train)}\")\n",
    "print(f\"testing accuracy: {rfr.score(x_test,y_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "id": "c0b2a3ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\S\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "model2=svc.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "id": "c6a954f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0,\n",
       "        0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0,\n",
       "        1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1,\n",
       "        0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0,\n",
       "        1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1,\n",
       "        1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0,\n",
       "        1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0,\n",
       "        1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "        1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1,\n",
       "        1, 0], dtype=int64),\n",
       " array([1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1,\n",
       "        0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1,\n",
       "        1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "        0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0,\n",
       "        1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1,\n",
       "        0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
       "        1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1,\n",
       "        0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1,\n",
       "        1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1,\n",
       "        1, 0], dtype=int64))"
      ]
     },
     "execution_count": 322,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y2_pred=model2.predict(x_test)\n",
    "y2_pred,y_test.values.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "id": "ec283d8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training accuracy: 0.99625\n",
      "testing accuracy: 0.765\n"
     ]
    }
   ],
   "source": [
    "print(f\"training accuracy: {svc.score(x_train,y_train)}\")\n",
    "print(f\"testing accuracy: {svc.score(x_test,y_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "id": "8e4829f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\S\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0,\n",
       "        0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0,\n",
       "        1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1,\n",
       "        0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1,\n",
       "        1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1,\n",
       "        1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0,\n",
       "        1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0,\n",
       "        1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1,\n",
       "        1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1,\n",
       "        1, 0], dtype=int64),\n",
       " array([1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1,\n",
       "        0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1,\n",
       "        1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "        0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0,\n",
       "        1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1,\n",
       "        0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
       "        1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1,\n",
       "        0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1,\n",
       "        1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1,\n",
       "        1, 0], dtype=int64))"
      ]
     },
     "execution_count": 324,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "lgr=LogisticRegression()\n",
    "model3=lgr.fit(x_train,y_train)\n",
    "y3_pred=lgr.predict(x_test)\n",
    "y3_pred,y_test.values.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "id": "1b7bb96b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training accuracy: 0.97375\n",
      "testing accuracy: 0.78\n"
     ]
    }
   ],
   "source": [
    "print(f\"training accuracy: {lgr.score(x_train,y_train)}\")\n",
    "print(f\"testing accuracy: {lgr.score(x_test,y_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "id": "7be0ec17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-0.16603819]),\n",
       " array([[ 0.2505989 ,  0.19463305,  0.56093744, ..., -0.16947458,\n",
       "          0.06866962, -0.60703243]]))"
      ]
     },
     "execution_count": 328,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgr.intercept_,lgr.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "id": "041a60ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.02837817, 0.97162183],\n",
       "       [0.08900271, 0.91099729],\n",
       "       [0.49146323, 0.50853677],\n",
       "       [0.17305239, 0.82694761],\n",
       "       [0.255208  , 0.744792  ],\n",
       "       [0.05704776, 0.94295224],\n",
       "       [0.0733257 , 0.9266743 ],\n",
       "       [0.0756365 , 0.9243635 ],\n",
       "       [0.82893259, 0.17106741],\n",
       "       [0.99195849, 0.00804151],\n",
       "       [0.62055948, 0.37944052],\n",
       "       [0.24169495, 0.75830505],\n",
       "       [0.84874108, 0.15125892],\n",
       "       [0.02924984, 0.97075016],\n",
       "       [0.51236723, 0.48763277],\n",
       "       [0.35080066, 0.64919934],\n",
       "       [0.11416376, 0.88583624],\n",
       "       [0.64064479, 0.35935521],\n",
       "       [0.25355851, 0.74644149],\n",
       "       [0.714687  , 0.285313  ],\n",
       "       [0.9741721 , 0.0258279 ],\n",
       "       [0.76582881, 0.23417119],\n",
       "       [0.69926314, 0.30073686],\n",
       "       [0.53198459, 0.46801541],\n",
       "       [0.06596658, 0.93403342],\n",
       "       [0.63751498, 0.36248502],\n",
       "       [0.55824512, 0.44175488],\n",
       "       [0.61094745, 0.38905255],\n",
       "       [0.19135741, 0.80864259],\n",
       "       [0.60998341, 0.39001659],\n",
       "       [0.80544409, 0.19455591],\n",
       "       [0.50654976, 0.49345024],\n",
       "       [0.17350791, 0.82649209],\n",
       "       [0.41321128, 0.58678872],\n",
       "       [0.57237443, 0.42762557],\n",
       "       [0.61924966, 0.38075034],\n",
       "       [0.68140032, 0.31859968],\n",
       "       [0.0521452 , 0.9478548 ],\n",
       "       [0.29914864, 0.70085136],\n",
       "       [0.69019021, 0.30980979],\n",
       "       [0.11200037, 0.88799963],\n",
       "       [0.59440757, 0.40559243],\n",
       "       [0.20746632, 0.79253368],\n",
       "       [0.55290887, 0.44709113],\n",
       "       [0.39752123, 0.60247877],\n",
       "       [0.81361142, 0.18638858],\n",
       "       [0.68630011, 0.31369989],\n",
       "       [0.83149189, 0.16850811],\n",
       "       [0.94931612, 0.05068388],\n",
       "       [0.48765673, 0.51234327],\n",
       "       [0.84934277, 0.15065723],\n",
       "       [0.12458864, 0.87541136],\n",
       "       [0.75957361, 0.24042639],\n",
       "       [0.22953002, 0.77046998],\n",
       "       [0.17186881, 0.82813119],\n",
       "       [0.32431054, 0.67568946],\n",
       "       [0.03375261, 0.96624739],\n",
       "       [0.65521225, 0.34478775],\n",
       "       [0.159921  , 0.840079  ],\n",
       "       [0.22858114, 0.77141886],\n",
       "       [0.82321956, 0.17678044],\n",
       "       [0.68951477, 0.31048523],\n",
       "       [0.45142393, 0.54857607],\n",
       "       [0.59609209, 0.40390791],\n",
       "       [0.78837811, 0.21162189],\n",
       "       [0.14797453, 0.85202547],\n",
       "       [0.95324579, 0.04675421],\n",
       "       [0.04857093, 0.95142907],\n",
       "       [0.88644647, 0.11355353],\n",
       "       [0.13954571, 0.86045429],\n",
       "       [0.48797419, 0.51202581],\n",
       "       [0.53240183, 0.46759817],\n",
       "       [0.47640387, 0.52359613],\n",
       "       [0.49257775, 0.50742225],\n",
       "       [0.64362768, 0.35637232],\n",
       "       [0.72905568, 0.27094432],\n",
       "       [0.44729875, 0.55270125],\n",
       "       [0.80957563, 0.19042437],\n",
       "       [0.67998609, 0.32001391],\n",
       "       [0.79809593, 0.20190407],\n",
       "       [0.32246457, 0.67753543],\n",
       "       [0.54141445, 0.45858555],\n",
       "       [0.1189567 , 0.8810433 ],\n",
       "       [0.42458062, 0.57541938],\n",
       "       [0.43180939, 0.56819061],\n",
       "       [0.58677943, 0.41322057],\n",
       "       [0.83002655, 0.16997345],\n",
       "       [0.47985892, 0.52014108],\n",
       "       [0.25996209, 0.74003791],\n",
       "       [0.17211671, 0.82788329],\n",
       "       [0.68224281, 0.31775719],\n",
       "       [0.7043428 , 0.2956572 ],\n",
       "       [0.67389853, 0.32610147],\n",
       "       [0.07962722, 0.92037278],\n",
       "       [0.54060873, 0.45939127],\n",
       "       [0.56057474, 0.43942526],\n",
       "       [0.15079179, 0.84920821],\n",
       "       [0.2103289 , 0.7896711 ],\n",
       "       [0.85180285, 0.14819715],\n",
       "       [0.58461733, 0.41538267],\n",
       "       [0.85525811, 0.14474189],\n",
       "       [0.57593488, 0.42406512],\n",
       "       [0.94347221, 0.05652779],\n",
       "       [0.0411036 , 0.9588964 ],\n",
       "       [0.04354452, 0.95645548],\n",
       "       [0.58570512, 0.41429488],\n",
       "       [0.98249377, 0.01750623],\n",
       "       [0.96428359, 0.03571641],\n",
       "       [0.59264511, 0.40735489],\n",
       "       [0.02616209, 0.97383791],\n",
       "       [0.4555443 , 0.5444557 ],\n",
       "       [0.35715356, 0.64284644],\n",
       "       [0.05889862, 0.94110138],\n",
       "       [0.62251151, 0.37748849],\n",
       "       [0.72920356, 0.27079644],\n",
       "       [0.13481092, 0.86518908],\n",
       "       [0.0526669 , 0.9473331 ],\n",
       "       [0.96335574, 0.03664426],\n",
       "       [0.084569  , 0.915431  ],\n",
       "       [0.36921411, 0.63078589],\n",
       "       [0.57580152, 0.42419848],\n",
       "       [0.10439754, 0.89560246],\n",
       "       [0.18870864, 0.81129136],\n",
       "       [0.00580863, 0.99419137],\n",
       "       [0.48267663, 0.51732337],\n",
       "       [0.21530651, 0.78469349],\n",
       "       [0.68141721, 0.31858279],\n",
       "       [0.89841706, 0.10158294],\n",
       "       [0.89598638, 0.10401362],\n",
       "       [0.34581857, 0.65418143],\n",
       "       [0.96386208, 0.03613792],\n",
       "       [0.85939099, 0.14060901],\n",
       "       [0.17211671, 0.82788329],\n",
       "       [0.39850276, 0.60149724],\n",
       "       [0.07686308, 0.92313692],\n",
       "       [0.80395513, 0.19604487],\n",
       "       [0.06814007, 0.93185993],\n",
       "       [0.85595044, 0.14404956],\n",
       "       [0.40866628, 0.59133372],\n",
       "       [0.4840496 , 0.5159504 ],\n",
       "       [0.02733144, 0.97266856],\n",
       "       [0.42817271, 0.57182729],\n",
       "       [0.26565742, 0.73434258],\n",
       "       [0.1932349 , 0.8067651 ],\n",
       "       [0.63959121, 0.36040879],\n",
       "       [0.21739339, 0.78260661],\n",
       "       [0.45502689, 0.54497311],\n",
       "       [0.87143535, 0.12856465],\n",
       "       [0.62858819, 0.37141181],\n",
       "       [0.02624013, 0.97375987],\n",
       "       [0.88894083, 0.11105917],\n",
       "       [0.75085494, 0.24914506],\n",
       "       [0.16844116, 0.83155884],\n",
       "       [0.60880261, 0.39119739],\n",
       "       [0.06660262, 0.93339738],\n",
       "       [0.64488753, 0.35511247],\n",
       "       [0.53554141, 0.46445859],\n",
       "       [0.49730001, 0.50269999],\n",
       "       [0.91852104, 0.08147896],\n",
       "       [0.30830126, 0.69169874],\n",
       "       [0.70482777, 0.29517223],\n",
       "       [0.59713155, 0.40286845],\n",
       "       [0.39675674, 0.60324326],\n",
       "       [0.74573527, 0.25426473],\n",
       "       [0.52715239, 0.47284761],\n",
       "       [0.77702022, 0.22297978],\n",
       "       [0.75121873, 0.24878127],\n",
       "       [0.49390814, 0.50609186],\n",
       "       [0.74831752, 0.25168248],\n",
       "       [0.87833776, 0.12166224],\n",
       "       [0.76736064, 0.23263936],\n",
       "       [0.80772287, 0.19227713],\n",
       "       [0.59987634, 0.40012366],\n",
       "       [0.45108529, 0.54891471],\n",
       "       [0.87407745, 0.12592255],\n",
       "       [0.23347934, 0.76652066],\n",
       "       [0.01032225, 0.98967775],\n",
       "       [0.03872095, 0.96127905],\n",
       "       [0.17211671, 0.82788329],\n",
       "       [0.12609786, 0.87390214],\n",
       "       [0.85146993, 0.14853007],\n",
       "       [0.39861794, 0.60138206],\n",
       "       [0.80894111, 0.19105889],\n",
       "       [0.48714658, 0.51285342],\n",
       "       [0.60938067, 0.39061933],\n",
       "       [0.05243922, 0.94756078],\n",
       "       [0.78909677, 0.21090323],\n",
       "       [0.15679729, 0.84320271],\n",
       "       [0.53943849, 0.46056151],\n",
       "       [0.34713284, 0.65286716],\n",
       "       [0.47855477, 0.52144523],\n",
       "       [0.36950188, 0.63049812],\n",
       "       [0.66845858, 0.33154142],\n",
       "       [0.46519821, 0.53480179],\n",
       "       [0.70619705, 0.29380295],\n",
       "       [0.69244349, 0.30755651],\n",
       "       [0.03521714, 0.96478286],\n",
       "       [0.39911494, 0.60088506],\n",
       "       [0.10330595, 0.89669405],\n",
       "       [0.64639747, 0.35360253]])"
      ]
     },
     "execution_count": 332,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result=lgr.predict_proba(x_test)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "id": "62549d1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\S\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "nb=GaussianNB()\n",
    "model4=nb.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "id": "ce1ce68a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1,\n",
       "        0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0,\n",
       "        0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0,\n",
       "        0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1,\n",
       "        1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1,\n",
       "        0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1,\n",
       "        1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1,\n",
       "        1, 1], dtype=int64),\n",
       " array([1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1,\n",
       "        0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1,\n",
       "        1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "        0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0,\n",
       "        1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1,\n",
       "        0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
       "        1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1,\n",
       "        0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1,\n",
       "        1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1,\n",
       "        1, 0], dtype=int64))"
      ]
     },
     "execution_count": 334,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y4_pred=model4.predict(x_test)\n",
    "y4_pred,y_test.values.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "id": "5aa37add",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training accuracy: 0.92375\n",
      "testing accuracy: 0.665\n"
     ]
    }
   ],
   "source": [
    "print(f\"training accuracy: {nb.score(x_train,y_train)}\")\n",
    "print(f\"testing accuracy: {nb.score(x_test,y_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "id": "b7f53ba1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0,\n",
       "        0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0,\n",
       "        0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1,\n",
       "        0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "        1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1,\n",
       "        0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "        1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "        1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "        1, 0], dtype=int64),\n",
       " array([1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1,\n",
       "        0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1,\n",
       "        1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "        0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0,\n",
       "        1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1,\n",
       "        0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
       "        1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1,\n",
       "        0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1,\n",
       "        1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1,\n",
       "        1, 0], dtype=int64))"
      ]
     },
     "execution_count": 336,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "dtc=DecisionTreeClassifier(criterion=\"entropy\",max_depth=15)\n",
    "model5=dtc.fit(x_train,y_train)\n",
    "y5_pred=dtc.predict(x_test)\n",
    "y5_pred,y_test.values.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "id": "fcee0e41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training accuracy: 0.80875\n",
      "testing accuracy: 0.71\n"
     ]
    }
   ],
   "source": [
    "print(f\"training accuracy: {dtc.score(x_train,y_train)}\")\n",
    "print(f\"testing accuracy: {dtc.score(x_test,y_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6225794e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
